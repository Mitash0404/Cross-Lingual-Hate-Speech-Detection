{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f34bb-eb18-4da2-8953-ff831fba12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d327d-6232-45e1-9390-6f804892bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e309f2c-f14f-4837-b260-0e92f7b77d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bf460d-0e24-4016-a94b-bb43eabb6d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model for XLM-Roberta\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eddfe28f-8a30-45a0-8a20-eaa7627a7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data (e.g., English HatEval)\n",
    "dataset = load_dataset(\"hate_speech18\", \"english\")  # Replace with your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eba12d4-e40a-4241-8712-834faaf1ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'user_id', 'subforum_id', 'num_contexts', 'label'],\n",
      "        num_rows: 10944\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ade28d7-9f7e-42ff-b404-4d1addc67302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize_data(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82198ba6-a3ca-4a91-bc50-2145e9cb6dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135db3d02f39472ab8339462a8c1f9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8755 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030537767bb845ef83bb109cb2acbd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2189 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# Get the new training and validation data\n",
    "train_data = train_test_split['train'].map(tokenize_data, batched=True)\n",
    "val_data = train_test_split['test'].map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d34cee3-8fbf-4adb-8354-1b37a044b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the datasets for PyTorch\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fe1947f-efd7-404b-b0d7-c3f540fc5b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbe8505b-2455-40fc-ac56-5a770412befb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3285' max='3285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3285/3285 1:47:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.375700</td>\n",
       "      <td>0.352421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.416200</td>\n",
       "      <td>0.363305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.404200</td>\n",
       "      <td>0.479198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3285, training_loss=0.3559436602135227, metrics={'train_runtime': 6466.7394, 'train_samples_per_second': 4.062, 'train_steps_per_second': 0.508, 'total_flos': 6910611869030400.0, 'train_loss': 0.3559436602135227, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97a25d26-919c-4cdb-8773-5f468cfbe96d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'hasoc_2019' doesn't exist on the Hub or cannot be accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load Hindi test data (e.g., HASOC 2019 Hindi) for zero-shot evaluation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hindi_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhasoc_2019\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhindi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/load.py:2074\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2069\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2070\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2071\u001b[0m )\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2074\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   2075\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   2076\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   2077\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   2078\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   2079\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   2080\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m   2081\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   2082\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   2083\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   2084\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   2085\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   2086\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m   2087\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2088\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[1;32m   2089\u001b[0m )\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/load.py:1795\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1793\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1794\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1795\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1796\u001b[0m     path,\n\u001b[1;32m   1797\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1798\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1799\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1800\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1801\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1802\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1803\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m   1804\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39m_require_default_config_name,\n\u001b[1;32m   1805\u001b[0m     _require_custom_configs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(config_kwargs),\n\u001b[1;32m   1806\u001b[0m )\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1808\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/load.py:1659\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[0;32m-> 1659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/load.py:1597\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[1;32m   1594\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1595\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hub or cannot be accessed.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_info\u001b[38;5;241m.\u001b[39mgated:\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'hasoc_2019' doesn't exist on the Hub or cannot be accessed."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd50ef24-476e-496e-8d34-039f180d3098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASOC 2019 Test Data Sample:\n",
      "         text_id                                               text task_1  \\\n",
      "0  hasoc_hi_5061  वक्त, इन्सान और इंग्लैंड का मौसम आपको कभी भी ध...    NOT   \n",
      "1  hasoc_hi_2090  #कांग्रेस के इस #कमीने की #करतूत को देखिए देश ...    HOF   \n",
      "2  hasoc_hi_2960  पाकिस्तान को फेकना था फेका गया। जो हार कर भी द...    HOF   \n",
      "3   hasoc_hi_864  जो शब्द तूम आज किसी और औरत के लिए यूज कर रहे व...    NOT   \n",
      "4    hasoc_hi_54  नेता जी हम समाजवादी सिपाही हमेशा आपके साथ है आ...    NOT   \n",
      "\n",
      "  task_2 task_3  \n",
      "0   NONE   NONE  \n",
      "1   OFFN    TIN  \n",
      "2   OFFN    TIN  \n",
      "3   NONE   NONE  \n",
      "4   NONE   NONE  \n",
      "\n",
      "HASOC 2019 Test Data Columns:\n",
      "Index(['text_id', 'text', 'task_1', 'task_2', 'task_3'], dtype='object')\n",
      "\n",
      "Hindi Dataset Sample:\n",
      "         text_id                                               text task_1  \\\n",
      "0  hasoc_hi_5556  बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT   \n",
      "1  hasoc_hi_5648  सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF   \n",
      "2   hasoc_hi_164  तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF   \n",
      "3  hasoc_hi_3530  बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT   \n",
      "4  hasoc_hi_5206  चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT   \n",
      "\n",
      "  task_2 task_3  \n",
      "0   NONE   NONE  \n",
      "1   PRFN    UNT  \n",
      "2   PRFN    TIN  \n",
      "3   NONE   NONE  \n",
      "4   NONE   NONE  \n",
      "\n",
      "Hindi Dataset Columns:\n",
      "Index(['text_id', 'text', 'task_1', 'task_2', 'task_3'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "hasoc_test = pd.read_csv('data/hindi_dataset/hasoc2019_hi_test_gold_2919.tsv', sep='\\t')\n",
    "hindi_data = pd.read_csv('data/hindi_dataset/hindi_dataset.tsv', sep='\\t')\n",
    "\n",
    "# Display the first few rows and columns of each dataset\n",
    "print(\"HASOC 2019 Test Data Sample:\")\n",
    "print(hasoc_test.head())\n",
    "print(\"\\nHASOC 2019 Test Data Columns:\")\n",
    "print(hasoc_test.columns)\n",
    "\n",
    "print(\"\\nHindi Dataset Sample:\")\n",
    "print(hindi_data.head())\n",
    "print(\"\\nHindi Dataset Columns:\")\n",
    "print(hindi_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c26dc044-675d-46a1-b05c-2df9c20b1358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  वक्त, इन्सान और इंग्लैंड का मौसम आपको कभी भी ध...      0\n",
      "1  #कांग्रेस के इस #कमीने की #करतूत को देखिए देश ...      1\n",
      "2  पाकिस्तान को फेकना था फेका गया। जो हार कर भी द...      1\n",
      "3  जो शब्द तूम आज किसी और औरत के लिए यूज कर रहे व...      0\n",
      "4  नेता जी हम समाजवादी सिपाही हमेशा आपके साथ है आ...      0\n",
      "                                                text  label\n",
      "0  बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...      0\n",
      "1  सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...      1\n",
      "2  तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...      1\n",
      "3  बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...      0\n",
      "4  चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the datasets\n",
    "hasoc_test = pd.read_csv('data/hindi_dataset/hasoc2019_hi_test_gold_2919.tsv', sep='\\t')\n",
    "hindi_data = pd.read_csv('data/hindi_dataset/hindi_dataset.tsv', sep='\\t')\n",
    "\n",
    "# Select only 'text' and 'task_1' columns for hate speech detection\n",
    "hasoc_test = hasoc_test[['text', 'task_1']]\n",
    "hindi_data = hindi_data[['text', 'task_1']]\n",
    "\n",
    "# Map 'HOF' to 1 and 'NOT' to 0 for easier numerical processing\n",
    "label_mapping = {'HOF': 1, 'NOT': 0}\n",
    "hasoc_test['label'] = hasoc_test['task_1'].map(label_mapping)\n",
    "hindi_data['label'] = hindi_data['task_1'].map(label_mapping)\n",
    "\n",
    "# Drop the original 'task_1' column now that we have the numerical labels\n",
    "hasoc_test = hasoc_test.drop(columns=['task_1'])\n",
    "hindi_data = hindi_data.drop(columns=['task_1'])\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(hasoc_test.head())\n",
    "print(hindi_data.head())\n",
    "\n",
    "# Convert to Hugging Face dataset format for compatibility with the model\n",
    "hasoc_test_dataset = Dataset.from_pandas(hasoc_test)\n",
    "hindi_dataset = Dataset.from_pandas(hindi_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fc225bc-999f-4cc7-beb9-34f01ce620b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d707a5b397004b86b0a91a1fd788286d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hasoc_test_dataset = hasoc_test_dataset.map(tokenize_data, batched=True)\n",
    "hasoc_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba5408b1-3af7-46f3-9373-49f5819bf408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/anaconda3/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (0.25.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "442fa711-cce3-4faa-8a3f-84f8ad2c21ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b246f2ebf74b3c90429eabfe5e33b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e61315c77341fea02616aec4cc4b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='83' max='83' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [83/83 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.7001206278800964, 'eval_model_preparation_time': 0.001, 'eval_f1': 0.31461258450338014, 'eval_accuracy': 0.4590288315629742, 'eval_runtime': 12.679, 'eval_samples_per_second': 103.951, 'eval_steps_per_second': 6.546}\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate  # New library for loading metrics\n",
    "\n",
    "# Load the pre-trained tokenizer (replace 'xlm-roberta-base' with your model if different)\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to tokenize the text data\n",
    "def tokenize_data(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Tokenize the test dataset\n",
    "hasoc_test_dataset = hasoc_test_dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "# Set format for PyTorch tensors for compatibility with Hugging Face models\n",
    "hasoc_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Load your trained model (replace 'path_to_your_model' with your actual model path)\n",
    "from transformers import XLMRobertaForSequenceClassification\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Define the evaluation metric using the evaluate library\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "# Define a function to compute metrics during evaluation\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = metric.compute(predictions=predictions, references=labels, average='macro')\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    return {\"f1\": f1['f1'], \"accuracy\": accuracy}\n",
    "\n",
    "# Define training arguments for evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer for evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=hasoc_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the model on the Hindi test data\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation Results: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f504916-afaf-47dc-8234-6a5a4e34a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the Hindi dataset\n",
    "results = trainer.evaluate(eval_dataset=hindi_test_data)\n",
    "print(f\"Zero-shot evaluation results on Hindi dataset: {results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
