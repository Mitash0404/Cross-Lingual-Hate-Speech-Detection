{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f34bb-eb18-4da2-8953-ff831fba12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d327d-6232-45e1-9390-6f804892bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e309f2c-f14f-4837-b260-0e92f7b77d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bf460d-0e24-4016-a94b-bb43eabb6d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model for XLM-Roberta\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eddfe28f-8a30-45a0-8a20-eaa7627a7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data (e.g., English HatEval)\n",
    "dataset = load_dataset(\"hate_speech18\", \"english\")  # Replace with your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eba12d4-e40a-4241-8712-834faaf1ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'user_id', 'subforum_id', 'num_contexts', 'label'],\n",
      "        num_rows: 10944\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ade28d7-9f7e-42ff-b404-4d1addc67302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize_data(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82198ba6-a3ca-4a91-bc50-2145e9cb6dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135db3d02f39472ab8339462a8c1f9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8755 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030537767bb845ef83bb109cb2acbd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2189 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# Get the new training and validation data\n",
    "train_data = train_test_split['train'].map(tokenize_data, batched=True)\n",
    "val_data = train_test_split['test'].map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d34cee3-8fbf-4adb-8354-1b37a044b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the datasets for PyTorch\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fe1947f-efd7-404b-b0d7-c3f540fc5b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbe8505b-2455-40fc-ac56-5a770412befb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3285' max='3285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3285/3285 1:47:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.375700</td>\n",
       "      <td>0.352421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.416200</td>\n",
       "      <td>0.363305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.404200</td>\n",
       "      <td>0.479198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3285, training_loss=0.3559436602135227, metrics={'train_runtime': 6466.7394, 'train_samples_per_second': 4.062, 'train_steps_per_second': 0.508, 'total_flos': 6910611869030400.0, 'train_loss': 0.3559436602135227, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97a25d26-919c-4cdb-8773-5f468cfbe96d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'hasoc_2019' doesn't exist on the Hub or cannot be accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load Hindi test data (e.g., HASOC 2019 Hindi) for zero-shot evaluation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hindi_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhasoc_2019\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhindi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/load.py:2074\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2069\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2070\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2071\u001b[0m )\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2074\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   2075\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   2076\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   2077\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   2078\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   2079\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   2080\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m   2081\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   2082\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   2083\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   2084\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   2085\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   2086\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m   2087\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2088\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[1;32m   2089\u001b[0m )\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/load.py:1795\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1793\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1794\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1795\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1796\u001b[0m     path,\n\u001b[1;32m   1797\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1798\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1799\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1800\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1801\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1802\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1803\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m   1804\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39m_require_default_config_name,\n\u001b[1;32m   1805\u001b[0m     _require_custom_configs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(config_kwargs),\n\u001b[1;32m   1806\u001b[0m )\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1808\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/load.py:1659\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[0;32m-> 1659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/load.py:1597\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[1;32m   1594\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1595\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hub or cannot be accessed.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_info\u001b[38;5;241m.\u001b[39mgated:\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'hasoc_2019' doesn't exist on the Hub or cannot be accessed."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd50ef24-476e-496e-8d34-039f180d3098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASOC 2019 Test Data Sample:\n",
      "         text_id                                               text task_1  \\\n",
      "0  hasoc_hi_5061  ‡§µ‡§ï‡•ç‡§§, ‡§á‡§®‡•ç‡§∏‡§æ‡§® ‡§î‡§∞ ‡§á‡§Ç‡§ó‡•ç‡§≤‡•à‡§Ç‡§° ‡§ï‡§æ ‡§Æ‡•å‡§∏‡§Æ ‡§Ü‡§™‡§ï‡•ã ‡§ï‡§≠‡•Ä ‡§≠‡•Ä ‡§ß...    NOT   \n",
      "1  hasoc_hi_2090  #‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡•á ‡§á‡§∏ #‡§ï‡§Æ‡•Ä‡§®‡•á ‡§ï‡•Ä #‡§ï‡§∞‡§§‡•Ç‡§§ ‡§ï‡•ã ‡§¶‡•á‡§ñ‡§ø‡§è ‡§¶‡•á‡§∂ ...    HOF   \n",
      "2  hasoc_hi_2960  ‡§™‡§æ‡§ï‡§ø‡§∏‡•ç‡§§‡§æ‡§® ‡§ï‡•ã ‡§´‡•á‡§ï‡§®‡§æ ‡§•‡§æ ‡§´‡•á‡§ï‡§æ ‡§ó‡§Ø‡§æ‡•§ ‡§ú‡•ã ‡§π‡§æ‡§∞ ‡§ï‡§∞ ‡§≠‡•Ä ‡§¶...    HOF   \n",
      "3   hasoc_hi_864  ‡§ú‡•ã ‡§∂‡§¨‡•ç‡§¶ ‡§§‡•Ç‡§Æ ‡§Ü‡§ú ‡§ï‡§ø‡§∏‡•Ä ‡§î‡§∞ ‡§î‡§∞‡§§ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡•Ç‡§ú ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§µ...    NOT   \n",
      "4    hasoc_hi_54  ‡§®‡•á‡§§‡§æ ‡§ú‡•Ä ‡§π‡§Æ ‡§∏‡§Æ‡§æ‡§ú‡§µ‡§æ‡§¶‡•Ä ‡§∏‡§ø‡§™‡§æ‡§π‡•Ä ‡§π‡§Æ‡•á‡§∂‡§æ ‡§Ü‡§™‡§ï‡•á ‡§∏‡§æ‡§• ‡§π‡•à ‡§Ü...    NOT   \n",
      "\n",
      "  task_2 task_3  \n",
      "0   NONE   NONE  \n",
      "1   OFFN    TIN  \n",
      "2   OFFN    TIN  \n",
      "3   NONE   NONE  \n",
      "4   NONE   NONE  \n",
      "\n",
      "HASOC 2019 Test Data Columns:\n",
      "Index(['text_id', 'text', 'task_1', 'task_2', 'task_3'], dtype='object')\n",
      "\n",
      "Hindi Dataset Sample:\n",
      "         text_id                                               text task_1  \\\n",
      "0  hasoc_hi_5556  ‡§¨‡§æ‡§Ç‡§ó‡•ç‡§≤‡§æ‡§¶‡•á‡§∂ ‡§ï‡•Ä ‡§∂‡§æ‡§®‡§¶‡§æ‡§∞ ‡§µ‡§æ‡§™‡§∏‡•Ä, ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•ã 314 ‡§∞‡§® ‡§™‡§∞ ...    NOT   \n",
      "1  hasoc_hi_5648  ‡§∏‡§¨ ‡§∞‡§Ç‡§°‡•Ä ‡§®‡§æ‡§ö ‡§¶‡•á‡§ñ‡§®‡•á ‡§Æ‡•á ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§ú‡•à‡§∏‡•á ‡§π‡•Ä ‡§ï‡•ã‡§à #‡§∂‡§æ‡§Ç‡§§‡•Ä...    HOF   \n",
      "2   hasoc_hi_164  ‡§§‡•Å‡§Æ ‡§ú‡•à‡§∏‡•á ‡§π‡§∞‡§æ‡§Æ‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§¨‡§∏ ‡§ú‡•Ç‡§§‡•ã‡§Ç ‡§ï‡•Ä ‡§ï‡§Æ‡•Ä ‡§π‡•à ‡§∂‡•Å...    HOF   \n",
      "3  hasoc_hi_3530  ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä MLA ‡§Ü‡§ï‡§æ‡§∂ ‡§µ‡§ø‡§ú‡§Ø‡§µ‡§∞‡•ç‡§ó‡•Ä‡§Ø ‡§ú‡•á‡§≤ ‡§∏‡•á ‡§∞‡§ø‡§π‡§æ, ‡§ú‡§Æ‡§æ‡§®‡§§ ...    NOT   \n",
      "4  hasoc_hi_5206  ‡§ö‡§Æ‡§ï‡•Ä ‡§¨‡•Å‡§ñ‡§æ‡§∞: ‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ ‡§™‡§∞‡§ø‡§∏‡§∞ ‡§Æ‡•á‡§Ç ‡§Ü‡§∞‡§ú‡•á‡§°‡•Ä ‡§ï‡§æ ‡§™‡•ç‡§∞‡§¶‡§∞...    NOT   \n",
      "\n",
      "  task_2 task_3  \n",
      "0   NONE   NONE  \n",
      "1   PRFN    UNT  \n",
      "2   PRFN    TIN  \n",
      "3   NONE   NONE  \n",
      "4   NONE   NONE  \n",
      "\n",
      "Hindi Dataset Columns:\n",
      "Index(['text_id', 'text', 'task_1', 'task_2', 'task_3'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "hasoc_test = pd.read_csv('data/hindi_dataset/hasoc2019_hi_test_gold_2919.tsv', sep='\\t')\n",
    "hindi_data = pd.read_csv('data/hindi_dataset/hindi_dataset.tsv', sep='\\t')\n",
    "\n",
    "# Display the first few rows and columns of each dataset\n",
    "print(\"HASOC 2019 Test Data Sample:\")\n",
    "print(hasoc_test.head())\n",
    "print(\"\\nHASOC 2019 Test Data Columns:\")\n",
    "print(hasoc_test.columns)\n",
    "\n",
    "print(\"\\nHindi Dataset Sample:\")\n",
    "print(hindi_data.head())\n",
    "print(\"\\nHindi Dataset Columns:\")\n",
    "print(hindi_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c26dc044-675d-46a1-b05c-2df9c20b1358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  ‡§µ‡§ï‡•ç‡§§, ‡§á‡§®‡•ç‡§∏‡§æ‡§® ‡§î‡§∞ ‡§á‡§Ç‡§ó‡•ç‡§≤‡•à‡§Ç‡§° ‡§ï‡§æ ‡§Æ‡•å‡§∏‡§Æ ‡§Ü‡§™‡§ï‡•ã ‡§ï‡§≠‡•Ä ‡§≠‡•Ä ‡§ß...      0\n",
      "1  #‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡•á ‡§á‡§∏ #‡§ï‡§Æ‡•Ä‡§®‡•á ‡§ï‡•Ä #‡§ï‡§∞‡§§‡•Ç‡§§ ‡§ï‡•ã ‡§¶‡•á‡§ñ‡§ø‡§è ‡§¶‡•á‡§∂ ...      1\n",
      "2  ‡§™‡§æ‡§ï‡§ø‡§∏‡•ç‡§§‡§æ‡§® ‡§ï‡•ã ‡§´‡•á‡§ï‡§®‡§æ ‡§•‡§æ ‡§´‡•á‡§ï‡§æ ‡§ó‡§Ø‡§æ‡•§ ‡§ú‡•ã ‡§π‡§æ‡§∞ ‡§ï‡§∞ ‡§≠‡•Ä ‡§¶...      1\n",
      "3  ‡§ú‡•ã ‡§∂‡§¨‡•ç‡§¶ ‡§§‡•Ç‡§Æ ‡§Ü‡§ú ‡§ï‡§ø‡§∏‡•Ä ‡§î‡§∞ ‡§î‡§∞‡§§ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡•Ç‡§ú ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§µ...      0\n",
      "4  ‡§®‡•á‡§§‡§æ ‡§ú‡•Ä ‡§π‡§Æ ‡§∏‡§Æ‡§æ‡§ú‡§µ‡§æ‡§¶‡•Ä ‡§∏‡§ø‡§™‡§æ‡§π‡•Ä ‡§π‡§Æ‡•á‡§∂‡§æ ‡§Ü‡§™‡§ï‡•á ‡§∏‡§æ‡§• ‡§π‡•à ‡§Ü...      0\n",
      "                                                text  label\n",
      "0  ‡§¨‡§æ‡§Ç‡§ó‡•ç‡§≤‡§æ‡§¶‡•á‡§∂ ‡§ï‡•Ä ‡§∂‡§æ‡§®‡§¶‡§æ‡§∞ ‡§µ‡§æ‡§™‡§∏‡•Ä, ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•ã 314 ‡§∞‡§® ‡§™‡§∞ ...      0\n",
      "1  ‡§∏‡§¨ ‡§∞‡§Ç‡§°‡•Ä ‡§®‡§æ‡§ö ‡§¶‡•á‡§ñ‡§®‡•á ‡§Æ‡•á ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§ú‡•à‡§∏‡•á ‡§π‡•Ä ‡§ï‡•ã‡§à #‡§∂‡§æ‡§Ç‡§§‡•Ä...      1\n",
      "2  ‡§§‡•Å‡§Æ ‡§ú‡•à‡§∏‡•á ‡§π‡§∞‡§æ‡§Æ‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§¨‡§∏ ‡§ú‡•Ç‡§§‡•ã‡§Ç ‡§ï‡•Ä ‡§ï‡§Æ‡•Ä ‡§π‡•à ‡§∂‡•Å...      1\n",
      "3  ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä MLA ‡§Ü‡§ï‡§æ‡§∂ ‡§µ‡§ø‡§ú‡§Ø‡§µ‡§∞‡•ç‡§ó‡•Ä‡§Ø ‡§ú‡•á‡§≤ ‡§∏‡•á ‡§∞‡§ø‡§π‡§æ, ‡§ú‡§Æ‡§æ‡§®‡§§ ...      0\n",
      "4  ‡§ö‡§Æ‡§ï‡•Ä ‡§¨‡•Å‡§ñ‡§æ‡§∞: ‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ ‡§™‡§∞‡§ø‡§∏‡§∞ ‡§Æ‡•á‡§Ç ‡§Ü‡§∞‡§ú‡•á‡§°‡•Ä ‡§ï‡§æ ‡§™‡•ç‡§∞‡§¶‡§∞...      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the datasets\n",
    "hasoc_test = pd.read_csv('data/hindi_dataset/hasoc2019_hi_test_gold_2919.tsv', sep='\\t')\n",
    "hindi_data = pd.read_csv('data/hindi_dataset/hindi_dataset.tsv', sep='\\t')\n",
    "\n",
    "# Select only 'text' and 'task_1' columns for hate speech detection\n",
    "hasoc_test = hasoc_test[['text', 'task_1']]\n",
    "hindi_data = hindi_data[['text', 'task_1']]\n",
    "\n",
    "# Map 'HOF' to 1 and 'NOT' to 0 for easier numerical processing\n",
    "label_mapping = {'HOF': 1, 'NOT': 0}\n",
    "hasoc_test['label'] = hasoc_test['task_1'].map(label_mapping)\n",
    "hindi_data['label'] = hindi_data['task_1'].map(label_mapping)\n",
    "\n",
    "# Drop the original 'task_1' column now that we have the numerical labels\n",
    "hasoc_test = hasoc_test.drop(columns=['task_1'])\n",
    "hindi_data = hindi_data.drop(columns=['task_1'])\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(hasoc_test.head())\n",
    "print(hindi_data.head())\n",
    "\n",
    "# Convert to Hugging Face dataset format for compatibility with the model\n",
    "hasoc_test_dataset = Dataset.from_pandas(hasoc_test)\n",
    "hindi_dataset = Dataset.from_pandas(hindi_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fc225bc-999f-4cc7-beb9-34f01ce620b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d707a5b397004b86b0a91a1fd788286d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hasoc_test_dataset = hasoc_test_dataset.map(tokenize_data, batched=True)\n",
    "hasoc_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba5408b1-3af7-46f3-9373-49f5819bf408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/anaconda3/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (0.25.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "442fa711-cce3-4faa-8a3f-84f8ad2c21ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b246f2ebf74b3c90429eabfe5e33b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e61315c77341fea02616aec4cc4b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='83' max='83' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [83/83 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.7001206278800964, 'eval_model_preparation_time': 0.001, 'eval_f1': 0.31461258450338014, 'eval_accuracy': 0.4590288315629742, 'eval_runtime': 12.679, 'eval_samples_per_second': 103.951, 'eval_steps_per_second': 6.546}\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate  # New library for loading metrics\n",
    "\n",
    "# Load the pre-trained tokenizer (replace 'xlm-roberta-base' with your model if different)\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to tokenize the text data\n",
    "def tokenize_data(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Tokenize the test dataset\n",
    "hasoc_test_dataset = hasoc_test_dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "# Set format for PyTorch tensors for compatibility with Hugging Face models\n",
    "hasoc_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Load your trained model (replace 'path_to_your_model' with your actual model path)\n",
    "from transformers import XLMRobertaForSequenceClassification\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Define the evaluation metric using the evaluate library\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "# Define a function to compute metrics during evaluation\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = metric.compute(predictions=predictions, references=labels, average='macro')\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    return {\"f1\": f1['f1'], \"accuracy\": accuracy}\n",
    "\n",
    "# Define training arguments for evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer for evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=hasoc_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the model on the Hindi test data\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation Results: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f504916-afaf-47dc-8234-6a5a4e34a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the Hindi dataset\n",
    "results = trainer.evaluate(eval_dataset=hindi_test_data)\n",
    "print(f\"Zero-shot evaluation results on Hindi dataset: {results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
