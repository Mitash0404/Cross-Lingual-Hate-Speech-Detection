{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1596269,"sourceType":"datasetVersion","datasetId":941744},{"sourceId":10216387,"sourceType":"datasetVersion","datasetId":6314970},{"sourceId":10214724,"sourceType":"datasetVersion","datasetId":6313612}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"Untitled0.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1886_zPsKL6c0aB6eOkV2nvSE6Y3r2BfH\n\"\"\"\n\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import XLMRobertaTokenizer, XLMRobertaModel, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.metrics import f1_score, precision_recall_fscore_support, classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve, auc\n\ndef set_seed(seed=42):\n    \"\"\"\n    Sets fixed random seeds for reproducibility.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # For deterministic behavior (may slow down training)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\n\nmr_sent_train = pd.read_csv('/kaggle/input/marathi/tweets-train.csv')\nmr_sent_test = pd.read_csv('/kaggle/input/marathi/tweets-test.csv')\nmr_sent_valid = pd.read_csv('/kaggle/input/marathi/tweets-valid.csv')\n\n# Combine Sentiment Datasets\ndataset2 = pd.concat([mr_sent_train, mr_sent_valid, mr_sent_test], axis=0, ignore_index=True)\ndataset2.rename(columns={'tweet': 'text'}, inplace=True)\nprint(\"Sentiment Dataset:\")\nprint(dataset2.head())\n\n# Load Hate Speech Datasets\nmr_maha_hate_train = pd.read_excel('/kaggle/input/marathi/hate_train.xlsx')\nmr_maha_hate_test = pd.read_excel('/kaggle/input/marathi/hate_test.xlsx')\nmr_maha_hate_val = pd.read_excel('/kaggle/input/marathi/hate_valid.xlsx')\n\n# Combine Hate Speech Datasets\ndataset1 = pd.concat([mr_maha_hate_train, mr_maha_hate_val, mr_maha_hate_test], axis=0, ignore_index=True)\nprint(\"\\nHate Speech Dataset:\")\nprint(dataset1.head())\n\n# Label Distribution in Hate Speech Dataset\nprint(\"\\nHate Speech Label Distribution:\")\nprint(dataset1['label'].value_counts())\n\n# Label Distribution in Sentiment Dataset\nprint(\"\\nSentiment Label Distribution:\")\nprint(dataset2['label'].value_counts())\n\n\n\n\n\n# Load Sentiment Analysis Datasets\nbn_sent_train = pd.read_csv('/kaggle/input/bangla/BN_data_train.tsv', sep='\\t')\nbn_sent_test = pd.read_csv('/kaggle/input/bangla/BN_data_test.tsv', sep='\\t')\nbn_sent_valid = pd.read_csv('/kaggle/input/bangla/BN_data_dev.tsv', sep='\\t')\n\n# Combine Sentiment Datasets\nbn_sent = pd.concat([bn_sent_train, bn_sent_valid, bn_sent_test], axis=0, ignore_index=True)\nbn_sent['label'] = bn_sent['class_label'].map({'BN_NEG':-1, 'BN_NEU':0, 'BN_POS':1})\nbn_sent = bn_sent.drop(columns=['class_label', 'id'])\nprint(\"Sentiment Dataset:\")\nprint(bn_sent.head())\n\n\n# Load Sentiment Analysis Datasets\nbn_hate_train = pd.read_csv('/kaggle/input/bangla/train.csv')\nbn_hate_test = pd.read_csv('/kaggle/input/bangla/test.csv')\nbn_hate_valid = pd.read_csv('/kaggle/input/bangla/val.csv')\n\n# Combine Sentiment Datasets\nbn_hate = pd.concat([bn_hate_train, bn_hate_valid, bn_hate_test], axis=0, ignore_index=True)\nbn_hate['label'] = bn_hate['hate speech'].map({1:'HATE', 0:'NOT'})\nbn_hate.rename(columns={'sentence': 'text'}, inplace=True)\nbn_hate = bn_hate.drop(columns=['target', 'type', 'hate speech'])\nprint(\"Hate Dataset:\")\nprint(bn_hate.head())\n\n\n\n# Load Sentiment Analysis Datasets\nbn_offn_train = pd.read_json('/kaggle/input/bangla/train.json')\nbn_offn_test = pd.read_json('/kaggle/input/bangla/test.json')\n\n# Combine Sentiment Datasets\nbn_offn = pd.concat([bn_offn_train, bn_offn_test], axis=0, ignore_index=True)\nbn_offn['label'] = bn_offn['offensive_gold'].map({'O':'OFFN', 'N':'NOT'})\nbn_offn = bn_offn.drop(columns=['code_mixed_gold', 'offensive_gold', 'target_gold'])\nprint(\"Offensive Dataset:\")\nprint(bn_offn.head())\n\n\n\n\ndef clean_text(text):\n    \"\"\"\n    Cleans the input text by removing URLs, mentions, hashtags, special characters, and extra whitespaces.\n    \"\"\"\n    # Lowercase the text\n    text = text.lower()\n\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Remove user @ references and '#' from hashtags\n    text = re.sub(r'\\@\\w+|\\#','', text)\n\n    # Remove extra whitespaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text\n\n\n# Apply cleaning to Hate Speech Dataset\ndataset1['clean_text'] = dataset1['text'].apply(clean_text)\n\n# Apply cleaning to Sentiment Dataset\ndataset2['clean_text'] = dataset2['text'].apply(clean_text)\n\nhate = dataset1[dataset1['label'] == 'HATE']\noffensive = dataset1[dataset1['label'] == 'OFFN']\nnot_label = dataset1[dataset1['label'] == 'NOT']\n\n# Determine the minimum number of samples among the classes\nmin_samples = min(len(hate), len(offensive), len(not_label)//2)\n\n# Combine to form a balanced dataset\nhate_dataset = pd.concat([hate[:min_samples], not_label[:min_samples]], axis=0).reset_index(drop=True)\noffensive_dataset = pd.concat([offensive[:min_samples], not_label[min_samples:2*min_samples]], axis=0).reset_index(drop=True)\n\nprint(\"\\nBalanced Hate Speech Dataset Label Distribution:\")\nprint(hate_dataset['label'].value_counts())\nprint(offensive_dataset['label'].value_counts())\n\n\n\n\n# Apply cleaning to Hate Speech Dataset\nbn_offn['clean_text'] = bn_offn['text'].apply(clean_text)\n\n# Apply cleaning to Sentiment Dataset\nbn_hate['clean_text'] = bn_hate['text'].apply(clean_text)\n\nbn_sent['clean_text'] = bn_sent['text'].apply(clean_text)\n\nprint(bn_hate['label'].value_counts())\nprint(bn_offn['label'].value_counts())\nprint(bn_sent['label'].value_counts())\n\n\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\nMAX_LEN = 128\n\n# ----------------------------\n# 5. Define Dataset Class\n# ----------------------------\nclass MultiTaskHateDataset(Dataset):\n    def __init__(self, hate_data, sentiment_data, offensive_data, tokenizer, max_len):\n        \"\"\"\n        Initializes the dataset with hate speech, offensive language, and sentiment data.\n\n        Parameters:\n        - hate_data: DataFrame containing hate speech and 'NOT' labels.\n        - sentiment_data: DataFrame containing sentiment labels.\n        - offensive_data: DataFrame containing offensive and 'NOT' labels.\n        - tokenizer: Tokenizer to encode the text.\n        - max_len: Maximum length for tokenization.\n        \"\"\"\n        self.hate_data = hate_data\n        self.sentiment_data = sentiment_data\n        self.offensive_data = offensive_data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n        # Combine the datasets for multi-task learning\n        self.data = self._create_multitask_data()\n\n    def _create_multitask_data(self):\n        \"\"\"\n        Combines hate, offensive, and sentiment datasets into a single DataFrame with task identifiers.\n        \"\"\"\n        # Add a task identifier to each dataset\n        ds_hate = self.hate_data.copy()\n        ds_hate['task'] = 'hate'\n        ds_hate = ds_hate.rename(columns={'label': 'task_label'})\n\n        ds_offensive = self.offensive_data.copy()\n        ds_offensive['task'] = 'offensive'\n        ds_offensive = ds_offensive.rename(columns={'label': 'task_label'})\n\n        ds_sentiment = self.sentiment_data.copy()\n        ds_sentiment['task'] = 'sentiment'\n        ds_sentiment = ds_sentiment.rename(columns={'label': 'task_label'})\n\n        # Concatenate the datasets\n        combined = pd.concat([ds_hate, ds_offensive, ds_sentiment], ignore_index=True)\n\n        return combined\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves the tokenized inputs and labels for a given index.\n        \"\"\"\n        row = self.data.iloc[idx]\n        text = row['clean_text']\n        task = row['task']\n        label = row['task_label']\n\n        # Tokenize the text\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        # Prepare the labels based on the task\n        if task == 'hate':\n            # Labels: 'HATE' -> 1, 'NOT' -> 0\n            label_map = {'HATE': 1, 'NOT': 0}\n            label = label_map.get(label, 0)  # Default to 'NOT' if not found\n\n        elif task == 'offensive':\n            # Labels: 'OFFN' -> 1, 'NOT' -> 0\n            label_map = {'OFFN': 1, 'NOT': 0}\n            label = label_map.get(label, 0)  # Default to 'NOT' if not found\n\n        elif task == 'sentiment':\n            # Labels: 'POSITIVE' -> 2, 'NEUTRAL' -> 1, 'NEGATIVE' -> 0\n            label_map = {-1: 0, 0: 1, 1: 2}\n            label = label_map.get(label, 1)  # Default to 'NEUTRAL' if not found\n\n        else:\n            label = -1  # Undefined task\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'task': task,\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\nmultitask_dataset = MultiTaskHateDataset(\n    hate_data=pd.concat([hate_dataset, bn_hate[bn_hate['label']=='NOT'][:2750], bn_hate[bn_hate['label']=='HATE'][:2750]]).sample(frac=1),          # Balanced Hate Speech Dataset\n    sentiment_data=pd.concat([bn_sent, dataset2[dataset2['label']==-1][:4000], dataset2[dataset2['label']==0][:4000], dataset2[dataset2['label']==1][:4000]]).sample(frac=1),         # Sentiment Dataset\n    offensive_data=pd.concat([offensive_dataset, bn_offn]).sample(frac=1), # Balanced Offensive Dataset\n    tokenizer=tokenizer,\n    max_len=MAX_LEN\n)\n\n# Split the combined data into training and testing sets\ntrain_size = 0.9\n# Stratify by both 'task' and 'task_label' to maintain label distribution across splits\ntrain_data, test_data = train_test_split(\n    multitask_dataset.data,\n    test_size=1 - train_size,\n    random_state=42,\n    stratify=multitask_dataset.data[['task', 'task_label']]\n)\n\n# Create training and testing datasets\ntrain_dataset = MultiTaskHateDataset(\n    hate_data=train_data[train_data['task'] == 'hate'],\n    sentiment_data=train_data[train_data['task'] == 'sentiment'],\n    offensive_data=train_data[train_data['task'] == 'offensive'],\n    tokenizer=tokenizer,\n    max_len=MAX_LEN\n)\n\ntest_dataset = MultiTaskHateDataset(\n    hate_data=test_data[test_data['task'] == 'hate'],\n    sentiment_data=test_data[test_data['task'] == 'sentiment'],\n    offensive_data=test_data[test_data['task'] == 'offensive'],\n    tokenizer=tokenizer,\n    max_len=MAX_LEN\n)\n\n# Verify Label Distribution in Splits\nprint(\"\\nTraining Set Label Distribution:\")\nprint(train_dataset.data['task'].value_counts())\n\nprint(\"\\nValidation Set Label Distribution:\")\nprint(test_dataset.data['task'].value_counts())\n\nBATCH_SIZE = 16\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nclass MultiTaskXLMR(nn.Module):\n    def __init__(self, model_name, num_labels_hate, num_labels_offensive, num_labels_sentiment):\n        \"\"\"\n        Initializes the multi-task model with separate classification heads.\n\n        Parameters:\n        - model_name: Pre-trained model name (e.g., 'xlm-roberta-base').\n        - num_labels_hate: Number of labels for hate task.\n        - num_labels_offensive: Number of labels for offensive task.\n        - num_labels_sentiment: Number of labels for sentiment task.\n        \"\"\"\n        super(MultiTaskXLMR, self).__init__()\n        self.encoder = XLMRobertaModel.from_pretrained(model_name)\n        hidden_size = self.encoder.config.hidden_size\n\n        # Classification head for hate task\n        self.classifier_hate = nn.Linear(hidden_size, num_labels_hate)\n\n        # Classification head for offensive task\n        self.classifier_offensive = nn.Linear(hidden_size, num_labels_offensive)\n\n        # Classification head for sentiment task\n        self.classifier_sentiment = nn.Linear(hidden_size, num_labels_sentiment)\n\n        # Dropout layer\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, input_ids, attention_mask, task):\n        \"\"\"\n        Forward pass for the model.\n\n        Parameters:\n        - input_ids: Tokenized input IDs.\n        - attention_mask: Attention masks.\n        - task: Task identifier ('hate', 'offensive', 'sentiment').\n\n        Returns:\n        - logits: Output logits for the specified task.\n        \"\"\"\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n        cls_output = self.dropout(cls_output)\n\n        if task == 'hate':\n            logits = self.classifier_hate(cls_output)\n        elif task == 'offensive':\n            logits = self.classifier_offensive(cls_output)\n        elif task == 'sentiment':\n            logits = self.classifier_sentiment(cls_output)\n        else:\n            raise ValueError(f\"Unknown task: {task}\")\n\n        return logits\n\nnum_labels_hate = 2          # 'HATE' -> 1, 'NOT' -> 0\nnum_labels_offensive = 2     # 'OFFN' -> 1, 'NOT' -> 0\nnum_labels_sentiment = 3     # 'NEGATIVE' -> 0, 'NEUTRAL' -> 1, 'POSITIVE' -> 2\n\n# Initialize the multi-task model\nmodel = MultiTaskXLMR(\n    model_name='xlm-roberta-base',\n    num_labels_hate=num_labels_hate,\n    num_labels_offensive=num_labels_offensive,\n    num_labels_sentiment=num_labels_sentiment\n)\n\n# ----------------------------\n# 9. Move Model to Device\n# ----------------------------\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel = model.to(device)\n\n# Compute class weights for hate task\nlabels_hate = train_dataset.hate_data['task_label'].map({'HATE': 1, 'NOT': 0}).values\nclass_weights_hate = compute_class_weight(class_weight='balanced', classes=np.unique(labels_hate), y=labels_hate)\nclass_weights_hate = torch.tensor(class_weights_hate, dtype=torch.float).to(device)\n\n# Compute class weights for offensive task\nlabels_offensive = train_dataset.offensive_data['task_label'].map({'OFFN': 1, 'NOT': 0}).values\nclass_weights_offensive = compute_class_weight(class_weight='balanced', classes=np.unique(labels_offensive), y=labels_offensive)\nclass_weights_offensive = torch.tensor(class_weights_offensive, dtype=torch.float).to(device)\n\n# Compute class weights for sentiment task\nlabels_sentiment = train_dataset.sentiment_data['task_label'].map({-1: 0, 0: 1, 1: 2}).values\nclass_weights_sentiment = compute_class_weight(class_weight='balanced', classes=np.unique(labels_sentiment), y=labels_sentiment)\nclass_weights_sentiment = torch.tensor(class_weights_sentiment, dtype=torch.float).to(device)\n\ncriterion_hate = nn.CrossEntropyLoss(weight=class_weights_hate)\ncriterion_offensive = nn.CrossEntropyLoss(weight=class_weights_offensive)\ncriterion_sentiment = nn.CrossEntropyLoss(weight=class_weights_sentiment)\n\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n\n# Total number of training steps\nEPOCHS = 10\ntotal_steps = len(train_loader) * EPOCHS\n\n# Define scheduler with warm-up steps\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(0.1 * total_steps),  # 10% of total steps for warm-up\n    num_training_steps=total_steps\n)\n\ndef train_epoch_multitask(\n    model,\n    data_loader,\n    optimizer,\n    device,\n    scheduler,\n    criterion_hate,\n    criterion_offensive,\n    criterion_sentiment\n):\n    \"\"\"\n    Trains the model for one epoch on the multi-task dataset.\n\n    Parameters:\n    - model: The multi-task model.\n    - data_loader: DataLoader for training data.\n    - optimizer: Optimizer.\n    - device: Device to run the model on.\n    - scheduler: Learning rate scheduler.\n    - criterion_hate: Loss function for hate task.\n    - criterion_offensive: Loss function for offensive task.\n    - criterion_sentiment: Loss function for sentiment task.\n\n    Returns:\n    - Average training loss for the epoch.\n    \"\"\"\n    model.train()\n    losses = []\n\n    for batch_idx, batch in enumerate(data_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        tasks = batch['task']\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n\n        # Identify unique tasks in the current batch\n        unique_tasks = set(tasks)\n\n        # Initialize total loss for the batch\n        total_loss = 0\n\n        for task in unique_tasks:\n            # Get indices for the current task\n            indices = [i for i, t in enumerate(tasks) if t == task]\n            if not indices:\n                continue  # Skip if no instances for the task\n\n            # Select inputs and labels for the current task\n            task_input_ids = input_ids[indices]\n            task_attention_mask = attention_mask[indices]\n            task_labels = labels[indices]\n\n            # Forward pass\n            logits = model(input_ids=task_input_ids, attention_mask=task_attention_mask, task=task)\n\n            # Compute loss based on the task\n            if task == 'hate':\n                loss = criterion_hate(logits, task_labels)\n            elif task == 'offensive':\n                loss = criterion_offensive(logits, task_labels)\n            elif task == 'sentiment':\n                loss = criterion_sentiment(logits, task_labels)\n            else:\n                raise ValueError(f\"Unknown task: {task}\")\n\n            # Accumulate loss\n            total_loss += loss\n\n        # Backward pass and optimization\n        total_loss.backward()\n\n        # Gradient Clipping to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n        # Record loss\n        losses.append(total_loss.item())\n\n        # Debugging: Print loss every 100 batches\n        if (batch_idx + 1) % 100 == 0:\n            print(f'Batch {batch_idx + 1}/{len(data_loader)} - Loss: {total_loss.item():.4f}')\n\n        # Optional: Check gradient flow\n        # Uncomment the following lines to verify gradients\n        # for name, param in model.named_parameters():\n        #     if param.requires_grad and param.grad is not None:\n        #         print(f\"Gradients for {name}: {param.grad.abs().mean()}\")\n\n    return np.mean(losses)\n\ndef eval_model_multitask(\n    model,\n    data_loader,\n    criterion_hate,\n    criterion_offensive,\n    criterion_sentiment,\n    device\n):\n    \"\"\"\n    Evaluates the model on the validation/test dataset.\n\n    Parameters:\n    - model: The multi-task model.\n    - data_loader: DataLoader for validation/test data.\n    - criterion_hate: Loss function for hate task.\n    - criterion_offensive: Loss function for offensive task.\n    - criterion_sentiment: Loss function for sentiment task.\n    - device: Device to run the model on.\n\n    Returns:\n    - Average loss, precision, recall, and F1 score for each task.\n    \"\"\"\n    model.eval()\n    losses = []\n    all_labels = {'hate': [], 'offensive': [], 'sentiment': []}\n    all_preds = {'hate': [], 'offensive': [], 'sentiment': []}\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            tasks = batch['task']\n            labels = batch['labels'].to(device)\n\n            # Identify unique tasks in the current batch\n            unique_tasks = set(tasks)\n\n            # Initialize total loss for the batch\n            total_loss = 0\n\n            for task in unique_tasks:\n                # Get indices for the current task\n                indices = [i for i, t in enumerate(tasks) if t == task]\n                if not indices:\n                    continue\n\n                # Select inputs and labels for the current task\n                task_input_ids = input_ids[indices]\n                task_attention_mask = attention_mask[indices]\n                task_labels = labels[indices]\n\n                # Forward pass\n                logits = model(input_ids=task_input_ids, attention_mask=task_attention_mask, task=task)\n\n                # Compute loss based on the task\n                if task == 'hate':\n                    loss = criterion_hate(logits, task_labels)\n                elif task == 'offensive':\n                    loss = criterion_offensive(logits, task_labels)\n                elif task == 'sentiment':\n                    loss = criterion_sentiment(logits, task_labels)\n                else:\n                    raise ValueError(f\"Unknown task: {task}\")\n\n                # Accumulate loss\n                total_loss += loss\n\n                # Predictions\n                preds = torch.argmax(logits, dim=1)\n\n                # Collect labels and predictions\n                all_labels[task].extend(task_labels.cpu().numpy())\n                all_preds[task].extend(preds.cpu().numpy())\n\n            # Record loss\n            losses.append(total_loss.item())\n\n    # Calculate metrics for each task\n    f1 = {}\n    precision = {}\n    recall = {}\n\n    for task in all_labels:\n        precision[task], recall[task], f1[task], _ = precision_recall_fscore_support(\n            all_labels[task], all_preds[task], average='weighted', zero_division=0\n        )\n\n    avg_loss = np.mean(losses)\n\n    return avg_loss, precision, recall, f1\n\ndef detailed_evaluation(model, data_loader, device):\n    \"\"\"\n    Generates detailed classification reports for each task.\n\n    Parameters:\n    - model: The multi-task model.\n    - data_loader: DataLoader for validation/test data.\n    - device: Device to run the model on.\n    \"\"\"\n    model.eval()\n    all_labels = {'hate': [], 'offensive': [], 'sentiment': []}\n    all_preds = {'hate': [], 'offensive': [], 'sentiment': []}\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            tasks = batch['task']\n            labels = batch['labels'].to(device)\n\n            # Identify unique tasks in the current batch\n            unique_tasks = set(tasks)\n\n            for task in unique_tasks:\n                # Get indices for the current task\n                indices = [i for i, t in enumerate(tasks) if t == task]\n                if not indices:\n                    continue\n\n                # Select inputs and labels for the current task\n                task_input_ids = input_ids[indices]\n                task_attention_mask = attention_mask[indices]\n                task_labels = labels[indices]\n\n                # Forward pass\n                logits = model(input_ids=task_input_ids, attention_mask=task_attention_mask, task=task)\n\n                # Predictions\n                preds = torch.argmax(logits, dim=1)\n\n                # Collect labels and predictions\n                all_labels[task].extend(task_labels.cpu().numpy())\n                all_preds[task].extend(preds.cpu().numpy())\n\n    # Generate classification reports\n    for task in all_labels:\n        print(f'Classification Report for {task.capitalize()}:')\n        if task == 'hate':\n            target_names = ['NOT', 'HATE']\n        elif task == 'offensive':\n            target_names = ['NOT', 'OFFN']\n        elif task == 'sentiment':\n            target_names = ['NEGATIVE', 'NEUTRAL', 'POSITIVE']  # {-1: 0, 0: 1, 1: 2}\n        else:\n            target_names = []\n        print(classification_report(all_labels[task], all_preds[task], target_names=target_names, zero_division=0))\n        print('-' * 50)\n\nbest_f1 = {'hate': 0, 'offensive': 0, 'sentiment': 0}\nloss_history = {'train': [], 'val': []}  # To store loss values for plotting\n\n# ----------------------------\n# 16. Training Loop\n# ----------------------------\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n\n    # Training Phase\n    train_loss = train_epoch_multitask(\n        model=model,\n        data_loader=train_loader,\n        optimizer=optimizer,\n        device=device,\n        scheduler=scheduler,\n        criterion_hate=criterion_hate,\n        criterion_offensive=criterion_offensive,\n        criterion_sentiment=criterion_sentiment\n    )\n\n    print(f'Train loss: {train_loss:.4f}')\n    loss_history['train'].append(train_loss)\n\n    # Evaluation Phase\n    val_loss, val_precision, val_recall, val_f1 = eval_model_multitask(\n        model=model,\n        data_loader=test_loader,\n        criterion_hate=criterion_hate,\n        criterion_offensive=criterion_offensive,\n        criterion_sentiment=criterion_sentiment,\n        device=device\n    )\n\n    print(f'Validation loss: {val_loss:.4f}')\n    print(f'Validation Precision: Hate: {val_precision[\"hate\"]:.4f}, Offensive: {val_precision[\"offensive\"]:.4f}, Sentiment: {val_precision[\"sentiment\"]:.4f}')\n    print(f'Validation Recall: Hate: {val_recall[\"hate\"]:.4f}, Offensive: {val_recall[\"offensive\"]:.4f}, Sentiment: {val_recall[\"sentiment\"]:.4f}')\n    print(f'Validation F1 Score: Hate: {val_f1[\"hate\"]:.4f}, Offensive: {val_f1[\"offensive\"]:.4f}, Sentiment: {val_f1[\"sentiment\"]:.4f}')\n    print()\n\n    loss_history['val'].append(val_loss)\n\n    # Check and save the best model based on F1 score\n    for task in best_f1:\n        torch.save(model.state_dict(), f'/kaggle/working/{epoch}.pth')\n        if val_f1[task] > best_f1[task]:\n            best_f1[task] = val_f1[task]\n            checkpoint_path = f'/kaggle/working/best_model_{task}.pth'\n            torch.save(model.state_dict(), checkpoint_path)\n            print(f\"Best model for task '{task}' saved to {checkpoint_path}\")\n\n# ----------------------------\n# 17. Save the Final Model and Components\n# ----------------------------\n\n# Define the path where you want to save the model\nmodel_save_path = '/kaggle/working/multi_task_xlm_roberta.pth'\n\n# Save the model's state_dict\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model state_dict saved to {model_save_path}\")\n\n# Define the path to save the tokenizer\ntokenizer_save_path = '/kaggle/working/tokenizer_xlm_roberta'\n\n# Save the tokenizer\ntokenizer.save_pretrained(tokenizer_save_path)\nprint(f\"Tokenizer saved to {tokenizer_save_path}\")\n\n# Save the optimizer state\noptimizer_save_path = '/kaggle/working/optimizer.pth'\ntorch.save(optimizer.state_dict(), optimizer_save_path)\nprint(f\"Optimizer state_dict saved to {optimizer_save_path}\")\n\n# Save the scheduler state\nscheduler_save_path = '/kaggle/working/scheduler.pth'\ntorch.save(scheduler.state_dict(), scheduler_save_path)\nprint(f\"Scheduler state_dict saved to {scheduler_save_path}\")\n\n# 18. Plot Training and Validation Losses\n# ----------------------------\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, EPOCHS + 1), loss_history['train'], label='Training Loss')\nplt.plot(range(1, EPOCHS + 1), loss_history['val'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Over Epochs')\nplt.legend()\nplt.show()\n\n# ----------------------------\n# 19. Load the Saved Model for Evaluation\n# ----------------------------\n\n# Define the path to save the tokenizer\ntokenizer_load_path = '/kaggle/working/tokenizer_xlm_roberta'\n# Load the tokenizer\ntokenizer = XLMRobertaTokenizer.from_pretrained(tokenizer_load_path)\nprint(f\"Tokenizer loaded from {tokenizer_load_path}\")\n\n# Load the saved state_dict\nmodel_load_path = '/kaggle/working/multi_task_xlm_roberta.pth'\nmodel.load_state_dict(torch.load(model_load_path, map_location=device))\nmodel.to(device)\nmodel.eval()\nprint(f\"Model loaded from {model_load_path}\")\n\n# Generate detailed classification reports\ndetailed_evaluation(model, test_loader, device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T15:18:40.047522Z","iopub.execute_input":"2024-12-16T15:18:40.047844Z","iopub.status.idle":"2024-12-16T16:10:14.354094Z","shell.execute_reply.started":"2024-12-16T15:18:40.047806Z","shell.execute_reply":"2024-12-16T16:10:14.352502Z"}},"outputs":[{"name":"stdout","text":"Sentiment Dataset:\n                                                text  label\n0    ज्येष्ठ पत्रकार अनंत दीक्षित यांच्या निधनाचे...     -1\n1    सर्वोच्च न्यायालयाचे निर्देश डावलून पुणे पोल...     -1\n2    उद्धव ठाकरेंनी भाजपासोबत युती करून शिवसैनिका...     -1\n3    आपला समाज खूप मोठा आहे. त्यात अनेक घटक अंतर्...      1\n4  बलात्काराचा बदला बलात्काराने घेतला पाहिजे हे म...     -1\n\nHate Speech Dataset:\n                                                text label\n0  @lokmat बहुसंख्यांक हिंदूंना घाबरट आणि मुर्ख ब...  HATE\n1  @BalaNandgaonkar स्वतःचे खिसे भरत आहेत, यांना ...  HATE\n2  या पुचाट UNO पेक्षा आमच्याकडचं ENO जास्त प्रभा...  HATE\n3  एक हिजाब परिधान केलेली मुलगी काल परवापासून व्ह...  HATE\n4  @KiritSomaiya @BJP4India @BJP4Maharashtra जंगी...  HATE\n\nHate Speech Label Distribution:\nlabel\nHATE    6250\nOFFN    6250\nPRFN    6250\nNOT     6250\nName: count, dtype: int64\n\nSentiment Label Distribution:\nlabel\n-1    5288\n 1    5288\n 0    5288\nName: count, dtype: int64\nSentiment Dataset:\n                                                text  label\n0  'অামি জেনে শুনে বিষ করেছি পান,প্রেমের ওঅাশা ছে...     -1\n1  'Photo: খুটাখালীতে হেডম্যান পুত্রকে গাছচোর সাজ...     -1\n2  'মঠবাড়িয়ায় পৌরশহরের পাকা সেতু বিধ্বস্তের দুই ব...     -1\n3  'দুটি পাবলিক পরীক্ষায়  এ+ পাওয়া সত্ত্বেও সবার ...     -1\n4  'রংপুর ও পাবনায় সড়ক দুর্ঘটনায় নিহত ৩:  http...     -1\nHate Dataset:\n                                                text label\n0                         .... ঐ ইন্দুর তোই মরছ নাই?  HATE\n1  #গেবনের শেষে আইসা আপনার মুখোশ টা খুলছে এতেই আম...  HATE\n2        ✈✈✈✈��� মুরগি চোরের পাছায় ডুকবি আর মারবি।।।  HATE\n3  ১৮ কোটির চোদা খাওয়া শেষে এখন ১৫০কোটির চোদা খাব...  HATE\n4                      ২য় মীর জাফরের মুখে মুতে দে...  HATE\nOffensive Dataset:\n                                                text label\n0                                eta ki kuno date??    NOT\n1   @Mehedi Hasan .  Notun SIM e prothom 54 taka ...   NOT\n2   @Sayed Islam .  Amader ei offer ti maximum 5 ...   NOT\n3   @Sumon .  amader shathe shorashori kotha bola...   NOT\n4   #Ajk Jdoi kono Hindo . Vuddo . Gristan ar upo...   NOT\n\nBalanced Hate Speech Dataset Label Distribution:\nlabel\nHATE    3125\nNOT     3125\nName: count, dtype: int64\nlabel\nOFFN    3125\nNOT     3125\nName: count, dtype: int64\nlabel\nNOT     26125\nHATE    24156\nName: count, dtype: int64\nlabel\nNOT     2619\nOFFN    2381\nName: count, dtype: int64\nlabel\n 0    368\n-1    353\n 1    276\nName: count, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40377ce4319e46e38d49d55a5023faf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d6f164b20c044e6ac961e980b32db69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bbae19a57d1406c982db2cdb7559a8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"747dfecc36cd4a1d924cc4d35599b4d6"}},"metadata":{}},{"name":"stdout","text":"\nTraining Set Label Distribution:\ntask\nsentiment    11697\nhate         10575\noffensive    10125\nName: count, dtype: int64\n\nValidation Set Label Distribution:\ntask\nsentiment    1300\nhate         1175\noffensive    1125\nName: count, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b98618adde94e3e8db5ac602a0ce0a4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n----------\nBatch 100/2025 - Loss: 2.8621\nBatch 200/2025 - Loss: 2.8879\nBatch 300/2025 - Loss: 2.4434\nBatch 400/2025 - Loss: 2.4017\nBatch 500/2025 - Loss: 2.0153\nBatch 600/2025 - Loss: 2.0803\nBatch 700/2025 - Loss: 1.7003\nBatch 800/2025 - Loss: 1.2466\nBatch 900/2025 - Loss: 1.3812\nBatch 1000/2025 - Loss: 1.5211\nBatch 1100/2025 - Loss: 2.3727\nBatch 1200/2025 - Loss: 0.9068\nBatch 1300/2025 - Loss: 1.1491\nBatch 1400/2025 - Loss: 1.2064\nBatch 1500/2025 - Loss: 2.0869\nBatch 1600/2025 - Loss: 1.3716\nBatch 1700/2025 - Loss: 1.8364\nBatch 1800/2025 - Loss: 1.1042\nBatch 1900/2025 - Loss: 1.6037\nBatch 2000/2025 - Loss: 2.0647\nTrain loss: 1.8472\nValidation loss: 0.5919\nValidation Precision: Hate: 0.8309, Offensive: 0.6954, Sentiment: 0.8147\nValidation Recall: Hate: 0.8187, Offensive: 0.6951, Sentiment: 0.8069\nValidation F1 Score: Hate: 0.8170, Offensive: 0.6947, Sentiment: 0.8050\n\nBest model for task 'hate' saved to /kaggle/working/best_model_hate.pth\nBest model for task 'offensive' saved to /kaggle/working/best_model_offensive.pth\nBest model for task 'sentiment' saved to /kaggle/working/best_model_sentiment.pth\nEpoch 2/10\n----------\nBatch 100/2025 - Loss: 1.1170\nBatch 200/2025 - Loss: 1.4633\nBatch 300/2025 - Loss: 2.1085\nBatch 400/2025 - Loss: 1.1027\nBatch 500/2025 - Loss: 1.9027\nBatch 600/2025 - Loss: 0.6062\nBatch 700/2025 - Loss: 0.7025\nBatch 800/2025 - Loss: 1.0268\nBatch 900/2025 - Loss: 0.7121\nBatch 1000/2025 - Loss: 0.3997\nBatch 1100/2025 - Loss: 0.6437\nBatch 1200/2025 - Loss: 1.2032\nBatch 1300/2025 - Loss: 1.1248\nBatch 1400/2025 - Loss: 2.6675\nBatch 1500/2025 - Loss: 1.8941\nBatch 1600/2025 - Loss: 1.7469\nBatch 1700/2025 - Loss: 1.2631\nBatch 1800/2025 - Loss: 0.5411\nBatch 1900/2025 - Loss: 0.7693\nBatch 2000/2025 - Loss: 1.8057\nTrain loss: 1.3392\nValidation loss: 0.4230\nValidation Precision: Hate: 0.8747, Offensive: 0.7905, Sentiment: 0.8140\nValidation Recall: Hate: 0.8740, Offensive: 0.7902, Sentiment: 0.8131\nValidation F1 Score: Hate: 0.8740, Offensive: 0.7901, Sentiment: 0.8134\n\nBest model for task 'hate' saved to /kaggle/working/best_model_hate.pth\nBest model for task 'offensive' saved to /kaggle/working/best_model_offensive.pth\nBest model for task 'sentiment' saved to /kaggle/working/best_model_sentiment.pth\nEpoch 3/10\n----------\nBatch 100/2025 - Loss: 0.3864\nBatch 200/2025 - Loss: 0.9556\nBatch 300/2025 - Loss: 1.4353\nBatch 400/2025 - Loss: 1.1009\nBatch 500/2025 - Loss: 0.9899\nBatch 600/2025 - Loss: 0.5972\nBatch 700/2025 - Loss: 0.8582\nBatch 800/2025 - Loss: 1.0072\nBatch 900/2025 - Loss: 2.8362\nBatch 1000/2025 - Loss: 1.0853\nBatch 1100/2025 - Loss: 2.0855\nBatch 1200/2025 - Loss: 1.6314\nBatch 1300/2025 - Loss: 0.5633\nBatch 1400/2025 - Loss: 1.0375\nBatch 1500/2025 - Loss: 1.5195\nBatch 1600/2025 - Loss: 0.3487\nBatch 1700/2025 - Loss: 0.2175\nBatch 1800/2025 - Loss: 0.9396\nBatch 1900/2025 - Loss: 0.3283\nBatch 2000/2025 - Loss: 0.6057\nTrain loss: 1.0590\nValidation loss: 0.4497\nValidation Precision: Hate: 0.8871, Offensive: 0.7968, Sentiment: 0.8056\nValidation Recall: Hate: 0.8868, Offensive: 0.7964, Sentiment: 0.8008\nValidation F1 Score: Hate: 0.8868, Offensive: 0.7963, Sentiment: 0.7997\n\nBest model for task 'hate' saved to /kaggle/working/best_model_hate.pth\nBest model for task 'offensive' saved to /kaggle/working/best_model_offensive.pth\nEpoch 4/10\n----------\nBatch 100/2025 - Loss: 1.1038\nBatch 200/2025 - Loss: 0.6600\nBatch 300/2025 - Loss: 0.7890\nBatch 400/2025 - Loss: 0.2869\nBatch 500/2025 - Loss: 0.1500\nBatch 600/2025 - Loss: 0.6430\nBatch 700/2025 - Loss: 1.1314\nBatch 800/2025 - Loss: 0.9066\nBatch 900/2025 - Loss: 0.9958\nBatch 1000/2025 - Loss: 0.2963\nBatch 1100/2025 - Loss: 0.1915\nBatch 1200/2025 - Loss: 0.4551\nBatch 1300/2025 - Loss: 1.2581\nBatch 1400/2025 - Loss: 0.2426\nBatch 1500/2025 - Loss: 0.7005\nBatch 1600/2025 - Loss: 1.1365\nBatch 1700/2025 - Loss: 0.7091\nBatch 1800/2025 - Loss: 0.9910\nBatch 1900/2025 - Loss: 1.4323\nBatch 2000/2025 - Loss: 1.0229\nTrain loss: 0.8510\nValidation loss: 0.4507\nValidation Precision: Hate: 0.9030, Offensive: 0.8197, Sentiment: 0.8105\nValidation Recall: Hate: 0.9030, Offensive: 0.8142, Sentiment: 0.8108\nValidation F1 Score: Hate: 0.9030, Offensive: 0.8131, Sentiment: 0.8106\n\nBest model for task 'hate' saved to /kaggle/working/best_model_hate.pth\nBest model for task 'offensive' saved to /kaggle/working/best_model_offensive.pth\nEpoch 5/10\n----------\nBatch 100/2025 - Loss: 0.4298\nBatch 200/2025 - Loss: 0.6961\nBatch 300/2025 - Loss: 0.2251\nBatch 400/2025 - Loss: 0.1013\nBatch 500/2025 - Loss: 2.1179\nBatch 600/2025 - Loss: 0.2771\nBatch 700/2025 - Loss: 0.4942\nBatch 800/2025 - Loss: 1.1932\nBatch 900/2025 - Loss: 0.0714\nBatch 1000/2025 - Loss: 1.5453\nBatch 1100/2025 - Loss: 0.3971\nBatch 1200/2025 - Loss: 2.2439\nBatch 1300/2025 - Loss: 0.5579\nBatch 1400/2025 - Loss: 0.0185\nBatch 1500/2025 - Loss: 0.4860\nBatch 1600/2025 - Loss: 0.8797\nBatch 1700/2025 - Loss: 0.0747\nBatch 1800/2025 - Loss: 1.8168\nBatch 1900/2025 - Loss: 0.0215\nBatch 2000/2025 - Loss: 0.5852\nTrain loss: 0.6725\nValidation loss: 0.5732\nValidation Precision: Hate: 0.8913, Offensive: 0.8255, Sentiment: 0.8193\nValidation Recall: Hate: 0.8902, Offensive: 0.8196, Sentiment: 0.8192\nValidation F1 Score: Hate: 0.8901, Offensive: 0.8184, Sentiment: 0.8170\n\nBest model for task 'offensive' saved to /kaggle/working/best_model_offensive.pth\nBest model for task 'sentiment' saved to /kaggle/working/best_model_sentiment.pth\nEpoch 6/10\n----------\nBatch 100/2025 - Loss: 0.0377\nBatch 200/2025 - Loss: 0.0236\nBatch 300/2025 - Loss: 0.3908\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 673\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# Training Phase\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch_multitask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion_hate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_hate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion_offensive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_offensive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion_sentiment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_sentiment\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    685\u001b[0m loss_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n","Cell \u001b[0;32mIn[1], line 490\u001b[0m, in \u001b[0;36mtrain_epoch_multitask\u001b[0;34m(model, data_loader, optimizer, device, scheduler, criterion_hate, criterion_offensive, criterion_sentiment)\u001b[0m\n\u001b[1;32m    487\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# Gradient Clipping to prevent exploding gradients\u001b[39;00m\n\u001b[1;32m    493\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# Load the saved state_dict\nmodel_load_path = '/kaggle/working/2.pth'\nmodel.load_state_dict(torch.load(model_load_path, map_location=device))\nmodel.to(device)\nmodel.eval()\nprint(f\"Model loaded from {model_load_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:10:19.472260Z","iopub.execute_input":"2024-12-16T16:10:19.473231Z","iopub.status.idle":"2024-12-16T16:10:20.501462Z","shell.execute_reply.started":"2024-12-16T16:10:19.473194Z","shell.execute_reply":"2024-12-16T16:10:20.500598Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/2644559369.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_load_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Model loaded from /kaggle/working/2.pth\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Define the path to your Hindi TSV files\nhindi_tsv_path1 = '/kaggle/input/hasoc2019/hasoc2019_hi_test_gold_2919.tsv'\nhindi_tsv_path2 = '/kaggle/input/hasoc2019/hindi_dataset.tsv'\n\n# Load the TSV files into pandas DataFrames\nhindi_df1 = pd.read_csv(hindi_tsv_path1, sep='\\t')  # Adjust 'sep' if different\nhindi_df2 = pd.read_csv(hindi_tsv_path2, sep='\\t')  # Adjust 'sep' if different\n\n# Concatenate the two DataFrames\nhindi_combined_df = pd.concat([hindi_df1, hindi_df2], axis=0, ignore_index=True)\nhindi_combined_df['label'] = hindi_combined_df['task_1'].map({'NOT': 'NOT', 'HOF': 'HATE'})\nprint(f\"Combined Hindi Dataset Shape: {hindi_combined_df.shape}\")\nhindi_combined_df['clean_text'] = hindi_combined_df['text'].apply(clean_text)\n\nhindi_combined_df = hindi_combined_df[['text', 'label', 'clean_text']]\n\n\nclass InferenceDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        \"\"\"\n        Initializes the dataset for inference.\n\n        Parameters:\n        - dataframe: pandas DataFrame containing the data.\n        - tokenizer: Tokenizer to encode the text.\n        - max_len: Maximum length for tokenization.\n        \"\"\"\n        self.texts = dataframe['clean_text'].tolist()\n        self.labels = dataframe['label'].tolist()\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves the tokenized inputs for a given index.\n        \"\"\"\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        # Tokenize the text\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'text': text,  # Optional: to keep track of the original text,\n            'label': label\n        }\n\n\n# Define maximum sequence length (should match training)\nMAX_LEN = 128\n\n# Instantiate the Inference Dataset\ninference_dataset = InferenceDataset(\n    dataframe=hindi_combined_df,\n    tokenizer=tokenizer,\n    max_len=MAX_LEN\n)\n\n# Create DataLoader\nBATCH_SIZE = 16\n\ninference_loader = DataLoader(\n    inference_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\ndef predict_tasks(model, data_loader, device):\n    \"\"\"\n    Runs inference on the data_loader and returns predictions for each task.\n\n    Parameters:\n    - model: The trained multi-task model.\n    - data_loader: DataLoader for inference data.\n    - device: Device to run the model on.\n\n    Returns:\n    - predictions: Dictionary containing predictions for each task.\n    \"\"\"\n    model.eval()\n    predictions = {'hate': [], 'offensive': [], 'sentiment': []}\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n\n            # For each task, perform inference separately\n            for task in ['hate', 'offensive', 'sentiment']:\n                logits = model(input_ids=input_ids, attention_mask=attention_mask, task=task)\n                preds = torch.argmax(logits, dim=1).cpu().numpy()\n                predictions[task].extend(preds)\n\n    return predictions\n\n# Run inference\npredictions = predict_tasks(model, inference_loader, device)\n\n# Display sample predictions\nprint(\"\\nSample Predictions:\")\nfor i in range(5):\n    print(f\"Text: {hindi_combined_df['text'].iloc[i]}\")\n    print(f\"True Label: {hindi_combined_df['label'].iloc[i]}\")\n    print(f\"Hate Prediction: {'HATE' if predictions['hate'][i] == 1 else 'NOT'}\")\n    print(f\"Offensive Prediction: {'OFFN' if predictions['offensive'][i] == 1 else 'NOT'}\")\n    sentiment_map = {0: 'NEGATIVE', 1: 'NEUTRAL', 2: 'POSITIVE'}\n    print(f\"Sentiment Prediction: {sentiment_map.get(predictions['sentiment'][i], 'UNKNOWN')}\")\n    print(\"-\" * 50)\n\n# Map hate predictions to labels\nhate_map = {0: 'NOT', 1: 'HATE'}\nhindi_combined_df['hate_prediction'] = predictions['hate']\nhindi_combined_df['hate_prediction'] = hindi_combined_df['hate_prediction'].map(hate_map)\n\n# Classification Report for Hate Task\nprint(\"Classification Report for Hate Task:\")\nprint(classification_report(hindi_combined_df['label'], hindi_combined_df['hate_prediction'], zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:10:24.789484Z","iopub.execute_input":"2024-12-16T16:10:24.790405Z","iopub.status.idle":"2024-12-16T16:11:27.074735Z","shell.execute_reply.started":"2024-12-16T16:10:24.790332Z","shell.execute_reply":"2024-12-16T16:11:27.073861Z"}},"outputs":[{"name":"stdout","text":"Combined Hindi Dataset Shape: (5983, 6)\n\nSample Predictions:\nText: वक्त, इन्सान और इंग्लैंड का मौसम आपको कभी भी धोखा दे सकता है।  #IndiaVsPakistan\nTrue Label: NOT\nHate Prediction: NOT\nOffensive Prediction: NOT\nSentiment Prediction: NEGATIVE\n--------------------------------------------------\nText: #कांग्रेस के इस #कमीने की #करतूत को देखिए देश की रक्षा करते #शहीद हुए मां भारती के #सपूतों का ख्याल भी नहीं रखा #दुश्मन से गले मिल गये  #शर्मनाक. ये पूरे शहीद का अपमान किया है। मैं बहुत कद्र करते थे सिंधु को लेकिन ये तो नकारा है।\nTrue Label: HATE\nHate Prediction: NOT\nOffensive Prediction: NOT\nSentiment Prediction: NEGATIVE\n--------------------------------------------------\nText: पाकिस्तान को फेकना था फेका गया। जो हार कर भी दुश्मन को हरा दे उसी को हमने नाम दिया। @BCCI  ये तुम्हारे समझ के बाहर हैं तुम बॉलीवुड में रंडी रोओ इधर मत घुसो।\nTrue Label: HATE\nHate Prediction: NOT\nOffensive Prediction: OFFN\nSentiment Prediction: NEGATIVE\n--------------------------------------------------\nText: जो शब्द तूम आज किसी और औरत के लिए यूज कर रहे वो बचाकर रखना क्योंकि यही कल कोई और कल तुम्हारी माँ बहन और बेटी के लिये यूज करेगा तब गुस्सा न करना क्योंकि वो तुम्हारे फेंके शब्द तुम्ही को मारेगा....  ये सलाय नहीं सच्चाई है आज के ट्विटर की  इतना गिरा दिया है कि पूछो मत\nTrue Label: NOT\nHate Prediction: NOT\nOffensive Prediction: NOT\nSentiment Prediction: NEGATIVE\n--------------------------------------------------\nText: नेता जी हम समाजवादी सिपाही हमेशा आपके साथ है आपका हर लिया गया निर्णय हमे स्वीकार है निवेदन है कि कार्यकर्ताओं पे वरिष्ठ नेता ध्यान दे ये चुनाव भी एक सबक है हमारे लिए कही न कही कार्यकर्ताओं को नजरअंदाज करना भी रख सबब बनाजय समाजवाद जय अखिलेश भइया\nTrue Label: NOT\nHate Prediction: NOT\nOffensive Prediction: NOT\nSentiment Prediction: POSITIVE\n--------------------------------------------------\nClassification Report for Hate Task:\n              precision    recall  f1-score   support\n\n        HATE       0.86      0.70      0.77      3074\n         NOT       0.73      0.88      0.80      2909\n\n    accuracy                           0.79      5983\n   macro avg       0.80      0.79      0.78      5983\nweighted avg       0.80      0.79      0.78      5983\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!zip -r 'bangla_marathi_final.zip' '/kaggle/working/'\nfrom IPython.display import FileLink\nFileLink('bangla_marathi_final.zip')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}